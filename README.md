# ğŸ¤– Personal Assistant Agent

A high-performance, scalable personal assistant built in Rust that runs entirely on your local machine using Ollama LLMs. No data leaves your computer - complete privacy and control.

## ğŸš€ What This Assistant Does

### Core Functionality
- **ğŸ’¬ Interactive Chat**: Natural conversations with local LLMs (qwen2.5:7b by default)
- **ğŸ§  Knowledge Learning**: Teach it information from text or files that it remembers
- **ğŸ” Smart Context**: Searches its knowledge base to provide relevant, contextual responses
- **ğŸ’¾ Conversation Memory**: Maintains chat history with configurable limits
- **ğŸ“ File Learning**: Can learn from any text file to expand its knowledge
- **ğŸ’¾ Conversation Saving**: Save and manage your chat sessions

### Architecture Overview
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CLI Interface â”‚â”€â”€â”€â”€â”‚   Assistant  â”‚â”€â”€â”€â”€â”‚  Ollama LLM     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Core       â”‚    â”‚  (qwen2.5:7b)  â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚         â”‚         â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ Knowledge â”‚ â”‚ Memory â”‚ â”‚ Embeddings  â”‚
            â”‚   Base    â”‚ â”‚Manager â”‚ â”‚  Service    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ¯ 2-Minute Demo Walkthrough

### What Happens When You Run It:

1. **ğŸ”§ Initialization**
   ```bash
   cargo run
   ```
   - Loads configuration (Ollama host, models, directories)
   - Creates data directories (`./data/knowledge_base`, `./data/conversations`)
   - Connects to local Ollama server
   - Verifies the LLM model is available

2. **ğŸ’¬ Chat Interface**
   ```
   You: hi there
   ğŸ¤– Assistant: Hello! How can I assist you today?
   ```
   - Your message goes through the conversation manager
   - System searches knowledge base for relevant context
   - Builds a prompt with system instructions + context + chat history
   - Sends to Ollama LLM for response
   - Saves both messages to conversation history

3. **ğŸ§  Teaching the Assistant**
   ```
   You: learn: Rust is a systems programming language focused on safety and performance
   âœ… Learned new information
   
   You: file: ./my_notes.txt
   âœ… Learned from ./my_notes.txt
   ```
   - Text gets chunked into manageable pieces (500 chars with 50 char overlap)
   - Each chunk becomes a Document with metadata
   - Generates embeddings using `nomic-embed-text` model
   - Stores in vector database for semantic search

4. **ğŸ” Smart Responses**
   ```
   You: tell me about Rust
   ğŸ¤– Assistant: Based on what I know, Rust is a systems programming language 
   focused on safety and performance... [uses learned context]
   ```
   - Searches knowledge base using semantic similarity
   - Finds relevant documents (top 3 by default)
   - Includes context in the prompt to LLM
   - Provides informed, contextual responses

## ğŸ¦€ Why Rust Over Python?

### Performance Benefits
- **âš¡ Speed**: 10-100x faster than Python for CPU-intensive tasks
- **ğŸ§µ Concurrency**: Excellent async/await support without GIL limitations
- **ğŸ’¾ Memory**: Zero-cost abstractions, no garbage collector overhead
- **ğŸ”§ Compilation**: Catches bugs at compile time, not runtime

### Production Readiness
- **ğŸ›¡ï¸ Safety**: Memory safety without garbage collection
- **ğŸ”’ Reliability**: Type system prevents common bugs (null pointers, data races)
- **ğŸ“¦ Deployment**: Single binary, no runtime dependencies
- **ğŸ”„ Scalability**: Handles thousands of concurrent operations efficiently

### AI/ML Ecosystem
- **ğŸ¤– Ollama Integration**: Excellent HTTP client libraries (reqwest)
- **ğŸ”¢ Vector Operations**: High-performance math libraries
- **ğŸ“Š Data Processing**: Fast text processing and serialization
- **ğŸ—„ï¸ Storage**: Embedded databases (sled) for vector storage

### Development Experience
- **ğŸ› ï¸ Tooling**: Cargo package manager, excellent IDE support
- **ğŸ“š Documentation**: Built-in docs, strong type hints
- **ğŸ§ª Testing**: Built-in testing framework
- **ğŸ” Debugging**: Excellent error messages and debugging tools

## ğŸ—ï¸ Technical Architecture

### Core Components

1. **Assistant Core** (`src/agent/assistant.rs`)
   - Main orchestrator that coordinates all components
   - Handles chat flow, knowledge management, conversation state

2. **LLM Integration** (`src/llm/`)
   - Ollama client for local LLM communication
   - Message formatting and response handling
   - Model availability checking

3. **Knowledge System** (`src/knowledge/`)
   - Document chunking and processing
   - Embedding generation using local models
   - Vector storage and semantic search

4. **Memory Management** (`src/memory/`)
   - Conversation history with configurable limits
   - Message persistence and retrieval
   - Context window management

5. **CLI Interface** (`src/cli/`)
   - Interactive terminal interface
   - Command parsing and execution
   - User-friendly error handling

### Data Flow
```
User Input â†’ Context Search â†’ Prompt Building â†’ LLM â†’ Response â†’ Memory Update
     â†“              â†‘              â†“              â†“         â†“          â†“
File Learning â†’ Embeddings â†’ Vector Store â†’ Context â†’ Display â†’ History
```

## ğŸš€ Getting Started

### Prerequisites
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve

# Pull required models
ollama pull qwen2.5:7b
ollama pull nomic-embed-text
```

### Running the Assistant
```bash
# Clone and run
git clone <your-repo>
cd assistant_agent
cargo run
```

### Available Commands
- **Chat**: Just type your message
- **Learn**: `learn: <information>` - Teach new facts
- **File**: `file: <path>` - Learn from a file
- **Save**: `save` - Save current conversation
- **Clear**: `clear` - Clear chat history
- **Quit**: `quit` or `exit` - Exit the program

## âš™ï¸ Configuration

Environment variables (optional):
```bash
export OLLAMA_HOST="http://localhost:11434"
export OLLAMA_MODEL="qwen2.5:7b"
export OLLAMA_EMBEDDING_MODEL="nomic-embed-text"
export OLLAMA_TEMPERATURE="0.7"
export DATA_DIR="./data"
```

## ğŸ“ Project Structure
```
src/
â”œâ”€â”€ agent/          # Core assistant logic
â”œâ”€â”€ cli/            # Command-line interface
â”œâ”€â”€ config/         # Configuration management
â”œâ”€â”€ knowledge/      # Knowledge base and embeddings
â”œâ”€â”€ llm/           # LLM integration (Ollama)
â”œâ”€â”€ memory/        # Conversation management
â””â”€â”€ utils/         # Utility functions
```

## ğŸ”® Future Enhancements
- Web interface option
- Multiple model support
- Advanced RAG techniques
- Plugin system
- Voice interface
- Multi-language support

## ğŸ“„ License
MIT License - Feel free to use and modify!